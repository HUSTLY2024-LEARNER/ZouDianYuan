# WinterMisson2
使用YOLOv8模型架构训练，需要自己编写config.yaml文件进行配置，可以选择适合的模型大小与大量的数据预处理操作。由于本地机器性能较差，只好选择s和n模型训练。在训练过程中发现训练速度和numworker与batchsize紧密相关，适当调整二者的值可以提高gpu利用率，最终比较了n模型50epoch到300epoch的结果与s模型50到200的结果，发现n模型效果不佳，故最终选择s模型。使用SGD优化器发现即使训练200epoch也效果不大（疑惑），所以没有采用网上所谓先adam后sgd的方式（可能要调参），全程使用adamW。经过比较，在模型100epoch左右时拟合效果较好（predict1），但依旧不理想（目标检测过于保守）。最后加大了预处理力度，在70epoch处拟合效果较好，也不理想😔（目标检测过于开放）。在网上学习加入了shuffle attention注意力机制，并自定义网络，但是效果仍旧不佳。  
一些已经进行过的尝试：  
- 打乱数据集 √  
- 大量应用数据增强 ×（或者需要更多的epoch）（但是要注意到极大的变形幅度极容易导致误判）  
- 使用yolo9c ×（太久）  
- 加入注意力shuffle attention（不知道有无影响）  
- 使用先Adam后sgd ×（或者需要调参）  
以后可以进行的尝试：  
- 加入slowfast架构魔改 （应该有效）（复杂度极高）  
- 使用余弦退火学习 （效果未知）（复杂度低）  
- 使用x等大模型训练 （效果差不多）（复杂度低）  
- 微调架构 （效果未知）（复杂度很高）  
- 使用大量仿真数据 （效果极好）（复杂度较高）
以后可以探索加入时序的神经网络，在加快速度的同时提升精度，也可以从数据集方面着手，设计更合理的数据增强方式或使用类似unity引擎获得大量虚拟数据预训练。